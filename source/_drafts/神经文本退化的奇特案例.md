---
title: 神经文本退化的奇特案例
tags:
---

# 神经文本退化的奇特案例

GPT-2生成高质量的文本，依赖于解码方法的随机性，主要是通过top-k采样，从最可能的k个选项中抽取一个，而不是可能性最大的解码文本。

事实上，针对高概率输出进行优化的解码策略，如beam search，也会导致文本出现退化。因为语言模型会给格式好的文本大高分，但较长文本的高分通常是通用的，重复的，笨拙的。也就是和人类的文本实际差异较大。如下图所示：

![image-20240519111712958](C:\Users\caoca\AppData\Roaming\Typora\typora-user-images\image-20240519111712958.png)

解决方法是Nucleus Sampling。核心关键是，每个time step中，主要的绝大部分概率集中在核中，核是一个词汇表小子集，一般范围在1到1000个候选项中。如果依赖固定的顶部top-k，或者使用温度参数控制分布的形状，而没有充分一直不可靠的尾部，这样效果不好。作者建议从top-p部分采样，动态扩展或收缩候选池。

简单说，最大化或者单纯用top-k采样，产生的文本可能性太大，虽然文本得分可能较高，但与人类写的文章差异很大，用词缺乏多样性。而且纯采样生成的好质量文章的概率很低。
